package com.hortonworks.spark.sql.hive.llap.util;

import java.io.IOException;
import java.sql.Connection;
import java.sql.SQLException;

import com.hortonworks.spark.sql.hive.llap.DefaultJDBCWrapper;
import com.hortonworks.spark.sql.hive.llap.HiveWarehouseSessionState;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.ql.ErrorMsg;
import org.apache.hive.service.cli.HiveSQLException;
import org.apache.spark.sql.execution.streaming.StreamMetadata;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import scala.Option;

/**
 * Cleaner for metadata generated by spark streaming at checkpoint location.
 * {@link StreamingMetaCleaner#clean()} does the cleaning.
 */
public class StreamingMetaCleaner {

  private static final Logger LOG = LoggerFactory.getLogger(StreamingMetaCleaner.class);
  private static final String EXACTLY_ONCE_COMMIT_KEY_FORMAT = "_meta_SPARK_%s";

  private HiveWarehouseSessionState sessionState;
  private String queryCheckpointDir;
  private String dbName;
  private String tableName;

  public StreamingMetaCleaner(HiveWarehouseSessionState sessionState, String queryCheckpointDir, String dbName, String tableName) {
    this.sessionState = sessionState;
    this.queryCheckpointDir = queryCheckpointDir;
    this.dbName = dbName;
    this.tableName = tableName;
  }

  /**
   * Cleans up following streaming metadata:
   * </br>
   * 1) Unsets table property of format {@link StreamingMetaCleaner#EXACTLY_ONCE_COMMIT_KEY_FORMAT} if exists.
   * <br/>
   * 2) Deletes given checkpoint directory.
   *
   * @throws IOException
   * @throws SQLException
   */
  public void clean() throws IOException, SQLException {
    Path queryCheckpointDirPath = new Path(queryCheckpointDir);
    Configuration hadoopConfiguration = sessionState.session.sparkContext().hadoopConfiguration();
    Option<StreamMetadata> streamMetadataOption = getStreamMetadata(hadoopConfiguration, queryCheckpointDirPath);
    if (streamMetadataOption.isEmpty()) {
      LOG.error("Unable to determine queryId, either queryCheckpointDir[{}] does not contain metadata file or it is corrupted!!!", queryCheckpointDir);
      return;
    }

    String exactlyOncePropertyKey = getExactlyOncePropertyKey(streamMetadataOption.get().id());
    try (Connection connection = DefaultJDBCWrapper.getConnector(sessionState)) {
      DefaultJDBCWrapper.unsetTableProperties(connection, dbName, tableName, false, exactlyOncePropertyKey);
      LOG.info("SUCCESSFUL: Unset tblproperty: {} for database:{}, table:{}", exactlyOncePropertyKey, dbName, tableName);
    } catch (HiveSQLException e) {
      if (ErrorMsg.ALTER_TBL_UNSET_NON_EXIST_PROPERTY.getErrorCode() == e.getErrorCode()) {
        LOG.info("Table property:{} not found for table:{}, proceeding to delete checkpoint directory.", exactlyOncePropertyKey, tableName);
      } else {
        throw e;
      }
    }
    queryCheckpointDirPath.getFileSystem(hadoopConfiguration).delete(queryCheckpointDirPath, true);
    LOG.info("SUCCESSFUL: Deleted specified queryCheckpointDir: {}", queryCheckpointDir);
  }

  private Option<StreamMetadata> getStreamMetadata(Configuration conf, Path queryCheckpointDirPath) throws IOException {
    FileSystem fs = queryCheckpointDirPath.getFileSystem(conf);
    Path qualifiedPath = queryCheckpointDirPath.makeQualified(fs.getUri(), fs.getWorkingDirectory());
    Path checkpointFile = new Path(qualifiedPath, "metadata");
    return StreamMetadata.read(checkpointFile, conf);
  }

  private String getExactlyOncePropertyKey(String queryId) {
    return String.format(EXACTLY_ONCE_COMMIT_KEY_FORMAT, queryId);
  }

}
